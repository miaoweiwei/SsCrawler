#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
@Time    : 2021/5/1 18:19
@Author  : miaozi
@File    : sspool_crawler.py
@Software: PyCharm
@Desc    : 
"""
import multiprocessing
from urllib import parse as urlparse
from urllib import request

import yaml

from crawler import UserAgentManager, Shadowsocks

HTTP_PROXY = "127.0.0.1:1088",  # ipåœ°å€ ip:port
HTTPS_PROXY = "127.0.0.1:1088"  # ipåœ°å€ ip:port


def default_ctor(loader, tag_suffix, node):
    return node.value


def download(url, params=None, method='get', user_agent=None, headers=None, is_local_proxy=True):
    """ ä¸‹è½½æŒ‡å®šurlçš„é¡µé¢å†…å®¹
    @param url: url
    @param params: å¦‚æœurlé‡Œå¸¦æœ‰äº†å‚æ•°ï¼Œé‚£ä¹ˆè¿™ä¸ªparamså°±ä¸èµ·ä½œç”¨
    @param method: è¯·æ±‚çš„æ–¹æ³•
    @param user_agent: user_agent
    @param headers: header è¯·æ±‚å¤´
    @param is_local_proxy: æ˜¯å¦ä½¿ç”¨æœ¬åœ°ä»£ç†
    @return: è¿”å›é¡µé¢çš„ä¸‹è½½å†…å®¹
    """
    if url is None:
        return None
    if headers is None:
        headers = {
            "Connection": "keep-alive",
            # "Host": multiprocessing.current_process().name,
            "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) coc_coc_browser/94.0.202 Chrome/88.0.4324.202 Safari/537.36",
        }
    if user_agent:
        headers["user-agent"] = user_agent
    if method == 'get':
        question_index = url.find("?")
        if question_index > 0:  # ? åé¢çš„å‚æ•°éƒ½éœ€è¦ç¼–ç 
            param_str = url[question_index + 1:]
            temp_params = param_str.split('&')
            param_str = ""
            for i, param in enumerate(temp_params):
                eq_index = param.find("=")
                if eq_index > 0:
                    param_str += urlparse.quote(param[:eq_index]) + "=" + urlparse.quote(param[eq_index + 1:])
                else:
                    param_str += urlparse.quote(param)
                if i < len(temp_params) - 1:
                    param_str += '&'
            url = url[:question_index] + "?" + param_str
        elif params:
            param_str = ""
            for j, item in enumerate(params.items()):
                param_str += urlparse.quote(item[0]) + '=' + urlparse.quote(item[1])
                if j < len(params) - 1:
                    param_str += "&"
            if param_str != '':
                url = urlparse.urljoin(url, '?' + param_str)
    elif method == 'post' and params:
        params = bytes(urlparse.urlencode(params), encoding='utf8')

    # ç»„ç»‡ request æ•°æ®
    if method == 'get':
        req = request.Request(url=url, headers=headers)
    elif params:  # ä¸æ˜¯ get è¯·æ±‚å°±ä½¿ç”¨ post å…ˆåˆ¤æ–­æœ‰æ²¡æœ‰å‚æ•°
        req = request.Request(url=url, data=params, headers=headers)
    else:
        req = request.Request(url=url, headers=headers)

    try:

        # å‘èµ·è¯·æ±‚
        # åˆ¤æ–­æ˜¯å¦éœ€è¦ä»£ç†è®¾ç½®
        if is_local_proxy:
            # 1åˆ›å»º ProxyHeader
            proxy_header = request.ProxyHandler(
                {
                    'http': HTTP_PROXY,  # ipåœ°å€ ip:port
                    'https': HTTPS_PROXY  # ipåœ°å€ ip:port
                }
            )
            # 2æ–°å»ºopenerå¯¹è±¡
            proxy_opener = request.build_opener(proxy_header)
            response = proxy_opener.open(req, timeout=100)
        else:
            response = request.urlopen(req, timeout=100)
        # ä¸ç­‰äº200è¯´æ˜è¯·æ±‚å¤±è´¥
        return response.read() if response.getcode() == 200 else None
    except Exception as ex:
        print("{0}ï¼š{1}".format(urlparse.urlparse(url).netloc, ex))
        return None


class SspoolCrawler(object):
    """çˆ¬å–https://sspool.herokuapp.com/ä¸Šçš„ Shadowsocks å…è´¹è´¦å·
        https://sspool.herokuapp.com/clash/proxies?c=CN,HK,TW&speed=15,30&type=ss

        æ¥å£å‚æ•°è¯´æ˜ï¼š
            ç±»å‹          type        ss,ssr,vmess,trojan	            å¯åŒæ—¶é€‰æ‹©å¤šä¸ªç±»å‹
            å›½å®¶          c           AT,CN,IN,HK,JP,NL,RU,SG,TW,US...	å¯åŒæ—¶é€‰æ‹©å¤šä¸ªå›½å®¶
            æ’é™¤å›½å®¶       nc          AT,CN,IN,HK,JP,NL,RU,SG,TW,US...	å¯åŒæ—¶é€‰æ‹©å¤šä¸ªå›½å®¶
            é€Ÿåº¦          speed       ä»»ä½•æ•°å­—                            å•ä¸ªæ•°å­—é€‰æ‹©æœ€ä½é€Ÿåº¦
                                                                        ä¸¤ä¸ªæ•°å­—é€‰æ‹©é€Ÿåº¦åŒºé—´
    """

    def __init__(self, uaManager, url=None, types={}, country={}, exclude_country={}, is_proxy=False):
        """ åˆ›å»ºä¸€ä¸ª Sspool çˆ¬è™«
        @param uaManager: useragent ç®¡ç†å™¨
        @param url: è¦çˆ¬å–çš„url
        @param types: æŠ“å–çš„ä»£ç†ç±»å‹
        @param country: æŠ“å–ssæ‰€åœ¨åœ°åŒº
        @param exclude_country: æ’é™¤çš„åœ°åŒº
        @param is_proxy: çˆ¬å–æ—¶æ˜¯å¦ä½¿ç”¨æœ¬åœ°ä»£ç†
        """
        self.uaManager = uaManager
        self.url = url
        self.types = types
        self.country = country
        self.exclude_country = exclude_country
        self.is_proxy = is_proxy

    def __format__(self, sspool_ss):
        """å¯¹ä»ssä»£ç†æ± ä¸­è·å–çš„ssè´¦æˆ·è¿›è¡Œè½¬æ¢æˆæœ¬åœ°çš„æ ¼å¼"""
        s = Shadowsocks()
        s.server = sspool_ss['server']
        s.server_port = sspool_ss['port']
        s.password = sspool_ss['password']
        s.method = sspool_ss['cipher']
        s.remarks = sspool_ss['name']
        return s

    def filter(self, ss):
        if self.types:
            if 'type' not in ss or ss['type'] not in self.types:
                return False
        if self.country:
            if 'country' in ss and ss['country'] not in self.country:
                return False
        if self.exclude_country:
            if 'country' in ss and ss['country'] in self.exclude_country:
                return False
        return True

    def crawl(self):
        ua = self.uaManager.get_user_agent_random()
        try:
            html_encode = download(self.url, user_agent=ua.user_agent, is_local_proxy=self.is_proxy)
            if html_encode:
                html_text = html_encode.decode()
                # è§£å†³ !<str> ä¹Ÿå°±æ˜¯yaml æ ‡ç­¾çš„é—®é¢˜
                yaml.add_multi_constructor('', default_ctor)
                sss_json = yaml.load(html_text, Loader=yaml.UnsafeLoader)
                # print(sss_json)
                # è¿‡æ»¤ é€‰æ‹©å…¶ä¸­çš„ Shadowsocks è´¦æˆ·
                if 'proxies' in sss_json and sss_json['proxies']:
                    return [self.__format__(ss) for ss in sss_json['proxies'] if self.filter(ss)]
        except Exception as ex:
            print("è¿›ç¨‹ï¼š{0} å‘ç”Ÿå¼‚å¸¸ï¼š{1}".format(multiprocessing.current_process().name, ex))
        return []


class ClashCrawler(object):
    def __init__(self, uaManager):
        self.url = "https://raw.githubusercontent.com/ssrsub/ssr/master/Clash.yml"
        self.uaManager = uaManager

    def __format__(self, clash_ss):
        """å¯¹ä»ssä»£ç†æ± ä¸­è·å–çš„ssè´¦æˆ·è¿›è¡Œè½¬æ¢æˆæœ¬åœ°çš„æ ¼å¼"""
        s = Shadowsocks()
        s.server = clash_ss['server']
        s.server_port = clash_ss['port']
        s.password = clash_ss['password']
        s.method = clash_ss['cipher']
        s.remarks = clash_ss['name']
        return s

    def crawl(self):
        ua = self.uaManager.get_user_agent_random()
        html_encode = download(self.url, user_agent=ua.user_agent, is_local_proxy=False)
        html_text = html_encode.decode()
        clash_json = yaml.load(html_text, Loader=yaml.SafeLoader)
        sss = {self.__format__(ss) for ss in clash_json['proxies'] if ss['type'] == 'ss'}
        return sss


if __name__ == '__main__':
    uaManager = UserAgentManager()
    url = "https://raw.githubusercontent.com/AzadNetCH/Clash/main/AzadNet.yml?type=ss,ssr&speed=10&nc=CN"
    url = "https://free886.herokuapp.com/clash/proxies?type=ssr,ss&speed=10&nc=CN"
    sspool_crawler = SspoolCrawler(uaManager, url, types='ss,ssr')
    data = sspool_crawler.crawl()
    print(data)

    # clash_crawler = ClashCrawler(uaManager)
    # sss = clash_crawler.crawl()
    # print(sss)

#     data_str = """port: 7890
# socks-port: 7891
# allow-lan: true
# proxies:
#   - {name: AE ğŸ‡¦ğŸ‡ª Sharjah @AzadNet, server: 109.169.72.249, port: 808, type: ss, cipher: chacha20-ietf-poly1305, password: G!yBwPWH3Vao, udp: true}
#   - {name: AZ ğŸ‡¦ğŸ‡¿ @AzadNet, server: 94.20.154.38, port: 50000, type: ss, cipher: aes-256-cfb, password: !<str> 3135771619, udp: true}
#   - {name: FR ğŸ‡«ğŸ‡· @AzadNet, server: 5.39.70.138, port: 2376, type: ss, cipher: aes-256-gcm, password: faBAoD54k87UJG7, udp: true}"""
#     yaml.add_multi_constructor('', default_ctor)
#     test_data = yaml.load(data_str)
#     print(test_data)
